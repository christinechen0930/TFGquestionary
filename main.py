import os
import re
import requests
import torch
import streamlit as st
from sentence_transformers import SentenceTransformer, util
import fitz  # PyMuPDF
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ====== é é¢è¨­å®š ======
st.set_page_config(page_title="ğŸŒ¿ ç¶ åœ’äº‹å‹™è©¢å•æ¬„", page_icon="ğŸŒ±", layout="centered")
os.makedirs("downloads", exist_ok=True)

# ====== æ¨¡å‹åŠ è¼‰ ======
@st.cache_resource
def load_model():
    return SentenceTransformer("sentence-transformers/multi-qa-MiniLM-L6-cos-v1")

model = load_model()

# ====== æ“·å–æœ€æ–°æ¶ˆæ¯ä¸­çš„å­é é¢ ======
def fetch_latest_news_links(keyword):
    base_url = "https://www.fg.tp.edu.tw"
    news_url = f"{base_url}/category/news/news1/"
    try:
        response = requests.get(news_url, timeout=10)
        response.raise_for_status()
    except Exception as e:
        return f"âŒ ç„¡æ³•é€£æ¥åˆ°æœ€æ–°æ¶ˆæ¯é é¢ï¼š{e}"

    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.find_all("a", href=True)

    matched_links = []
    for a in articles:
        title = a.get_text(strip=True)
        href = a["href"]
        if keyword in title and href.startswith("/news/"):
            full_url = urljoin(base_url, href)
            matched_links.append(full_url)

    if not matched_links:
        return f"âŒ æ‰¾ä¸åˆ°ç¬¦åˆã€Œ{keyword}ã€çš„å­é é¢ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚"

    return matched_links

# ====== æ“·å–ç¶²é æ–‡å­—å…§å®¹ ======
def extract_text_from_url(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except Exception as e:
        return f"âŒ ç„¡æ³•é€£æ¥åˆ°ç¶²é ï¼š{e}"

    soup = BeautifulSoup(response.text, "html.parser")
    content_div = soup.find("div", class_="entry-content")
    if not content_div:
        return "âŒ ç„¡æ³•æ‰¾åˆ°ç¶²é å…§å®¹ã€‚"

    paragraphs = content_div.stripped_strings
    return "\n".join(paragraphs)

# ====== ä¸‹è¼‰ä¸¦è®€å– PDF æª”æ¡ˆ ======
def download_and_read_pdfs(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except Exception as e:
        return [], f"âŒ ç„¡æ³•é€£æ¥åˆ°ç¶²é ï¼š{e}"

    soup = BeautifulSoup(response.text, "html.parser")
    pdf_links = soup.find_all("a", href=re.compile(r".*\.pdf$"))

    texts = []
    for link in pdf_links:
        pdf_url = urljoin(url, link["href"])
        try:
            pdf_response = requests.get(pdf_url, timeout=10)
            pdf_response.raise_for_status()
            pdf_filename = os.path.join("downloads", os.path.basename(pdf_url))
            with open(pdf_filename, "wb") as f:
                f.write(pdf_response.content)

            doc = fitz.open(pdf_filename)
            for page in doc:
                texts.append(page.get_text())
            doc.close()
        except Exception as e:
            texts.append(f"âŒ ç„¡æ³•ä¸‹è¼‰æˆ–è®€å– PDFï¼š{e}")

    return texts, None

# ====== æ¸…ç†æ–‡å­— ======
def clean_and_split_text(text):
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"ç¬¬\s*\d+\s*é ", "", text)
    paragraphs = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text)
    return [p.strip() for p in paragraphs if len(p.strip()) > 10]

# ====== æ‰¾åˆ°ç›¸é—œæ®µè½ ======
def retrieve_relevant_content(task, paragraphs):
    paragraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)
    query_embedding = model.encode(task, convert_to_tensor=True)
    scores = util.pytorch_cos_sim(query_embedding, paragraph_embeddings)[0]

    top_k = min(10, len(paragraphs))
    top_results = torch.topk(scores, k=top_k)
    return "\n".join([paragraphs[idx] for idx in top_results.indices])

# ====== æ•´åˆå›ç­” ======
def generate_response_combined(task, keyword):
    if not keyword.strip():
        return "âŒ è«‹è¼¸å…¥é—œéµå­—"

    links = fetch_latest_news_links(keyword)
    if isinstance(links, str):
        return links

    all_texts = []
    for link in links:
        page_text = extract_text_from_url(link)
        pdf_texts, error = download_and_read_pdfs(link)
        if error:
            return error
        all_texts.extend([page_text] + pdf_texts)

    paragraphs = []
    for text in all_texts:
        paragraphs.extend(clean_and_split_text(text))

    if not paragraphs:
        return "âŒ æ‰¾ä¸åˆ°èˆ‡å•é¡Œç›¸é—œçš„å…§å®¹ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚"

    relevant_content = retrieve_relevant_content(task, paragraphs)
    if not relevant_content.strip():
        return "âŒ æ‰¾ä¸åˆ°èˆ‡å•é¡Œç›¸é—œçš„å…§å®¹ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚"

    source_links = "\n".join([f"- [ä¾†æºé é¢]({link})" for link in links])

    response = f"""
### ğŸ” å•é¡Œï¼š{task}

{relevant_content}

---

### ğŸ“„ ä¾†æºé é¢
{source_links}
"""
    return response

# ====== Streamlit ä»‹é¢ ======
st.title("ğŸŒ± ç¶ åœ’äº‹å‹™è©¢å•æ¬„")

task = st.text_input("è¼¸å…¥è©¢å•äº‹é …", "ä¾‹å¦‚ï¼šå¦‚ä½•ç”³è«‹äº¤æ›å­¸ç”Ÿï¼Ÿ")
keyword = st.text_input("è¼¸å…¥é—œéµå­—ï¼ˆè‡ªå‹•æœå°‹åŒ—ä¸€å¥³æœ€æ–°æ¶ˆæ¯ï¼‰", "ä¾‹å¦‚ï¼šç•¢æ¥­å…¸ç¦®")

if st.button("ç”Ÿæˆå›ç­”"):
    with st.spinner('æ­£åœ¨è™•ç†...'):
        response = generate_response_combined(task, keyword)
        st.success('è™•ç†å®Œæˆï¼')
        st.markdown(response)

st.markdown("---")
if st.button("ç­è§£åŒ—ä¸€å¥³æ ¡å²"):
    js = "window.open('https://christinechen0930.github.io/TFGquestionary/TFGhistory.html')"
    st.components.v1.html(f"<script>{js}</script>", height=0, width=0)
