file_path = "/mnt/data/green_garden_inquiry.py"
with open(file_path, "w", encoding="utf-8") as f:
    f.write("""import streamlit as st
import requests
import os
import torch
from sentence_transformers import SentenceTransformer, util
import fitz  # PyMuPDF
import docx  # éœ€è¦å®‰è£ python-docx
from tavily import TavilyClient

# è¨­å®š Tavily API Keyï¼ˆè«‹æ›¿æ›æˆä½ çš„ API Keyï¼‰
API_KEY = "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"
tavily_client = TavilyClient(api_key=API_KEY)

# å»ºç«‹ä¸‹è¼‰è³‡æ–™å¤¾
DOWNLOAD_FOLDER = "downloads"
os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)

# åŠ è¼‰èªç¾©æœå°‹æ¨¡å‹
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

def search_and_download_pdfs(keyword):
    \"\"\"æ ¹æ“šé—œéµå­—æœå°‹åŒ—ä¸€å¥³æ ¡ç¶² PDF ä¸¦ä¸‹è¼‰\"\"\"
    query = f"site:fg.tp.edu.tw {keyword} filetype:pdf"
    response = tavily_client.search(query)
    pdf_links = [result["url"] for result in response.get("results", []) if result["url"].endswith(".pdf")]

    if not pdf_links:
        return "âŒ æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ PDF æª”æ¡ˆï¼"

    pdf_paths = []
    for index, pdf_url in enumerate(pdf_links):
        try:
            response = requests.get(pdf_url, timeout=10)
            pdf_filename = os.path.join(DOWNLOAD_FOLDER, f"{keyword}_{index + 1}.pdf")
            with open(pdf_filename, "wb") as f:
                f.write(response.content)
            pdf_paths.append(pdf_filename)
        except Exception as e:
            return f"âŒ ä¸‹è¼‰å¤±æ•—ï¼š{pdf_url}ï¼ŒéŒ¯èª¤ï¼š{e}"

    return pdf_paths

def read_pdf(file_path):
    \"\"\"å¾ PDF æ–‡ä»¶ä¸­æå–æ–‡å­—\"\"\"
    try:
        doc = fitz.Document(file_path)
        full_text = [page.get_text() for page in doc]
        return full_text
    except Exception as e:
        return [f"è®€å– PDF æ–‡ä»¶æ™‚å‡ºç¾éŒ¯èª¤ï¼š{str(e)}"]

def read_docx(file):
    \"\"\"å¾ DOCX æ–‡ä»¶ä¸­æå–æ–‡å­—\"\"\"
    try:
        doc = docx.Document(file.name)
        full_text = [para.text.strip() for para in doc.paragraphs if para.text.strip()]
        return full_text
    except Exception as e:
        return [f"è®€å– DOCX æ–‡ä»¶æ™‚å‡ºç¾éŒ¯èª¤ï¼š{str(e)}"]

def retrieve_relevant_content(task, paragraphs):
    \"\"\"æ ¹æ“šä»»å‹™éœ€æ±‚åŸ·è¡Œèªç¾©æœå°‹ï¼Œä»¥æª¢ç´¢ç›¸é—œå…§å®¹\"\"\"
    paragraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)
    query_embedding = model.encode(task, convert_to_tensor=True)
    scores = util.pytorch_cos_sim(query_embedding, paragraph_embeddings)[0]
    top_k = min(5, len(paragraphs))
    top_results = torch.topk(scores, k=top_k)

    relevant_paragraphs = [paragraphs[idx] for idx in top_results.indices]
    return " ".join(relevant_paragraphs)

def generate_response_combined(task, keyword, file):
    \"\"\"æ ¹æ“šé—œéµå­—æœå°‹æ ¡ç¶² PDF æˆ–è™•ç†ä¸Šå‚³çš„æ–‡ä»¶ä¾†å›ç­”å•é¡Œã€‚\"\"\"
    
    if file:
        # **æœ‰ä¸Šå‚³æª”æ¡ˆæ™‚ï¼Œç›´æ¥è§£ææª”æ¡ˆ**
        if file.name.endswith(".pdf"):
            paragraphs = read_pdf(file)
        elif file.name.endswith(".docx"):
            paragraphs = read_docx(file)
        else:
            return "âŒ æ–‡ä»¶æ ¼å¼ä¸æ”¯æ´ï¼Œè«‹ä¸Šå‚³ PDF æˆ– DOCXï¼"
    else:
        # **æ²’æœ‰ä¸Šå‚³æª”æ¡ˆæ™‚ï¼Œè‡ªå‹•æœå°‹æ ¡ç¶² PDF**
        if not keyword.strip():
            return "âŒ è«‹è¼¸å…¥é—œéµå­—æˆ–ä¸Šå‚³æ–‡ä»¶ï¼"
        
        pdf_paths = search_and_download_pdfs(keyword)
        if isinstance(pdf_paths, str):  # è‹¥æœå°‹å¤±æ•—ï¼Œå›å‚³éŒ¯èª¤è¨Šæ¯
            return pdf_paths
        
        # è®€å–ä¸‹è¼‰çš„ PDF
        paragraphs = []
        for pdf_path in pdf_paths:
            paragraphs.extend(read_pdf(pdf_path))
    
    if not paragraphs or (isinstance(paragraphs[0], str) and "éŒ¯èª¤" in paragraphs[0]):
        return paragraphs[0]  # è¿”å›éŒ¯èª¤è¨Šæ¯
    
    # **ä½¿ç”¨èªç¾©æœå°‹ç¢ºä¿å›ç­”ç²¾æº–**
    relevant_content = retrieve_relevant_content(task, paragraphs)
    if not relevant_content.strip():
        return "âŒ æ‰¾ä¸åˆ°èˆ‡å•é¡Œç›¸é—œçš„è³‡è¨Šï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚"

    # **ä½¿ç”¨ Gemini API ç”Ÿæˆå›ç­”**
    prompt = f\"\"\"
    è«‹æ ¹æ“šä»¥ä¸‹æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œï¼š
    å•é¡Œï¼š{task}
    ç›¸é—œå…§å®¹ï¼š
    {relevant_content}

    è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ç”¨æ¢åˆ—å¼æˆ–ç°¡çŸ­æ‘˜è¦è¡¨é”ï¼Œé¿å…ç„¡é—œè³‡è¨Šã€‚
    \"\"\"

    api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
    api_key = "AIzaSyC25eTdPDzuMqv3ZE_I8l6gpuv0faBA88c"

    headers = {"Content-Type": "application/json"}
    payload = {"contents": [{"parts": [{"text": prompt}]}]}

    response = requests.post(f"{api_url}?key={api_key}", json=payload, headers=headers)
    
    if response.status_code == 200:
        response_json = response.json()
        if "candidates" in response_json and len(response_json["candidates"]) > 0:
            return response_json["candidates"][0]["content"]["parts"][0]["text"]
        else:
            return "âŒ ç„¡æ³•æ‰¾åˆ°ç”Ÿæˆçš„æ–‡æœ¬"
    else:
        return f"âŒ éŒ¯èª¤: {response.status_code}, {response.text}"

# Streamlit éƒ¨åˆ†
st.title("ğŸŒ± ç¶ åœ’äº‹å‹™è©¢å•æ¬„")

task = st.text_input("è¼¸å…¥è©¢å•äº‹é …", "ä¾‹å¦‚ï¼šå¦‚ä½•ç”³è«‹äº¤æ›å­¸ç”Ÿï¼Ÿ")
keyword = st.text_input("è¼¸å…¥é—œéµå­—ï¼ˆè‡ªå‹•æœå°‹åŒ—ä¸€å¥³ PDFï¼‰", "ä¾‹å¦‚ï¼šæ‹›ç”Ÿç°¡ç« ")
file_input = st.file_uploader("æˆ–ä¸Šå‚³ PDF / DOCX", type=["pdf", "docx"])

if st.button("ç”Ÿæˆå›ç­”"):
    response = generate_response_combined(task, keyword, file_input)
    st.markdown(response)""")

print(f"æª”æ¡ˆå·²å­˜å…¥: {file_path}")