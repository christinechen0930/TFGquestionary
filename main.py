import requests
from bs4 import BeautifulSoup
import os
from datetime import datetime, timedelta
import time

# === è¨­å®š ===
BASE_URL = "https://www.fg.tp.edu.tw"
NEWS_URL = f"{BASE_URL}/news"
DOWNLOAD_FOLDER = "downloads"
DAYS_LIMIT = 90  # åªæŠ“æœ€è¿‘å¹¾å¤©çš„å…¬å‘Š

# å»ºç«‹ä¸‹è¼‰è³‡æ–™å¤¾
os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)

# å°å·¥å…·ï¼šå½è£ç€è¦½å™¨ï¼Œé¿å…è¢«æ“‹
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
}

# å°å·¥å…·ï¼šçˆ¬ä¸€é å…¬å‘Šåˆ—è¡¨
def fetch_announcements(url):
    resp = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(resp.text, "html.parser")

    announcements = []
    today = datetime.today()
    cutoff_date = today - timedelta(days=DAYS_LIMIT)

    # é€™é‚Šè¦é‡å°åŒ—ä¸€å¥³å…¬å‘Šçš„çµæ§‹èª¿æ•´
    news_items = soup.select(".news-item")  # å‡è¨­æ˜¯é€™å€‹classï¼Œè¨˜å¾—ç¢ºèª
    if not news_items:  # é˜²å‘†
        print("âš ï¸ æŠ“ä¸åˆ°å…¬å‘Šï¼Œå¯èƒ½classåç¨±è¦èª¿æ•´")

    for item in news_items:
        title_tag = item.select_one(".title")
        date_tag = item.select_one(".date")
        link_tag = item.select_one("a")

        if not title_tag or not date_tag or not link_tag:
            continue

        title = title_tag.text.strip()
        date_str = date_tag.text.strip()
        link = link_tag["href"]

        # æ—¥æœŸè½‰æ›ï¼ˆåŒ—ä¸€å¥³æ˜¯è¥¿å…ƒæ ¼å¼ï¼Ÿï¼‰
        try:
            date = datetime.strptime(date_str, "%Y-%m-%d")
        except:
            print(f"âŒ æ—¥æœŸè§£æå¤±æ•—: {date_str}")
            continue

        if date >= cutoff_date:
            announcements.append({
                "title": title,
                "date": date,
                "link": BASE_URL + link if not link.startswith("http") else link
            })

    return announcements

# å°å·¥å…·ï¼šä¸‹è¼‰å…¬å‘Šè£¡çš„é™„ä»¶
def download_attachments_from_announcement(announcement):
    resp = requests.get(announcement["link"], headers=HEADERS)
    soup = BeautifulSoup(resp.text, "html.parser")

    file_links = []

    for a in soup.select("a"):
        href = a.get("href", "")
        if any(href.lower().endswith(ext) for ext in [".pdf", ".docx", ".doc", ".pptx", ".ppt"]):
            file_url = href if href.startswith("http") else BASE_URL + href
            file_links.append(file_url)

    for file_url in file_links:
        file_name = os.path.join(DOWNLOAD_FOLDER, os.path.basename(file_url))

        try:
            file_resp = requests.get(file_url, headers=HEADERS)
            with open(file_name, "wb") as f:
                f.write(file_resp.content)
            print(f"âœ… æˆåŠŸä¸‹è¼‰: {file_name}")
        except Exception as e:
            print(f"âŒ ä¸‹è¼‰å¤±æ•—: {file_url}, éŒ¯èª¤: {e}")

# === ä¸»æµç¨‹ ===
def main():
    print(f"ğŸš€ é–‹å§‹å¾åŒ—ä¸€å¥³ä¸­æ ¡ç¶²æŠ“æœ€è¿‘ {DAYS_LIMIT} å¤©å…§æœ‰é™„ä»¶ã„‰å…¬å‘Š...")

    announcements = fetch_announcements(NEWS_URL)

    print(f"ğŸ¯ ç¸½å…±æ‰¾åˆ° {len(announcements)} ç­†å…¬å‘Š")

    for ann in announcements:
        print(f"ğŸ” è™•ç†å…¬å‘Šï¼š{ann['title']} ({ann['date'].strftime('%Y-%m-%d')})")
        download_attachments_from_announcement(ann)
        time.sleep(1)  # ç¦®è²Œä¸€é»ï¼Œæ…¢æ…¢ä¾†é¿å…è¢«å°XD

    print("ğŸ å…¨éƒ¨ä¸‹è¼‰å®Œæˆã„Œï¼")

if __name__ == "__main__":
    main()
