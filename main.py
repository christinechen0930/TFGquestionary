import os
import re
import requests
import torch
import streamlit as st
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote, urlparse
from sentence_transformers import SentenceTransformer, util
import fitz  # PyMuPDF

"""
ç¶ åœ’äº‹å‹™è©¢å•æ¬„ v2
- ä¿®æ­£ï¼šfetch_relevant_news_page ç„¡æ³•æ‰¾åˆ°ã€è¡Œäº‹æ›†ã€ç­‰æ–‡ç« â€”â€”åŸå› æ˜¯æœ€æ–°æ¶ˆæ¯åˆ—è¡¨ä»¥ JS å‹•æ…‹è¼‰å…¥ï¼Œrequests æ‹¿ä¸åˆ°ï¼Œæ‰€ä»¥æ”¹æ¡ WordPress REST API + Tavily æœå°‹ã€‚
- æ–°å¢ï¼šè‹¥ keyword æœ¬èº«å°±æ˜¯ https é–‹é ­ã„‰å®Œæ•´ç¶²å€ï¼Œç›´æ¥ç•¶ page_url ç”¨ã€‚
- å…¶é¤˜ï¼šä¿ç•™åŒç¾©è©å°ç…§è¡¨ & æ—¢æœ‰æµç¨‹ã€‚
"""

# ====== è¨­å®š API Key ======
TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]

# ====== é é¢è¨­å®š ======
st.set_page_config(page_title="ğŸŒ¿ ç¶ åœ’äº‹å‹™è©¢å•æ¬„", page_icon="ğŸŒ±", layout="centered")
os.makedirs("downloads", exist_ok=True)

# ====== åŒç¾©è©å°ç…§è¡¨ ======
SYNONYMS = {
    "æ®µè€ƒ": ["æœŸä¸­è€ƒ", "æœŸæœ«è€ƒ"],
    "æœŸä¸­": ["æœŸä¸­è€ƒ"],
    "æœŸæœ«": ["æœŸæœ«è€ƒ"],
    "ç•¢æ¥­å…¸ç¦®": ["ç•¢å…¸", "ç•¢æ¥­å…¸ç¦®"],
    # å¯æ“´å……æ›´å¤š
}

# ====== æ¨¡å‹åŠ è¼‰ ======
@st.cache_resource
def load_model():
    return SentenceTransformer("sentence-transformers/multi-qa-MiniLM-L6-cos-v1")

model = load_model()

# ====== å·¥å…·å‡½å¼ ======
def clean_and_split_text(text: str):
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"ç¬¬\s*\d+\s*é ", "", text)
    paragraphs = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ])", text)
    return [p.strip() for p in paragraphs if len(p.strip()) > 10]


def read_pdf(file_path: str):
    try:
        doc = fitz.Document(file_path)
        paras = []
        for page in doc:
            paras.extend(clean_and_split_text(page.get_text()))
        return paras
    except Exception as e:
        return [f"è®€å– PDF éŒ¯èª¤ï¼š{e}"]

# ====== å–å¾—æœ€æ–°æ¶ˆæ¯æ–‡ç« ç¶²å€ ======

def _search_wordpress_rest(keyword: str):
    """ç”¨ WP REST API æœå°‹ï¼ŒæˆåŠŸå›å‚³ç¬¬ä¸€ç­†ç¶²å€ï¼Œå¤±æ•—å› None"""
    base = "https://www.fg.tp.edu.tw/wp-json/wp/v2/search"
    try:
        r = requests.get(base, params={"search": keyword, "per_page": 1}, timeout=10)
        if r.ok and r.json():
            return r.json()[0]["url"]
    except Exception:
        pass
    return None


def _search_tavily(keyword: str):
    """ç”¨ Tavily site: æœå°‹ï¼ŒæˆåŠŸå›å‚³ç¬¬ä¸€ç­†ç¶²å€"""
    try:
        resp = requests.post(
            "https://api.tavily.com/search",
            headers={"Authorization": TAVILY_API_KEY},
            json={"query": f"site:fg.tp.edu.tw {keyword}", "l": 1},
            timeout=10,
        )
        data = resp.json()
        if data and data.get("results"):
            return data["results"][0]["url"]
    except Exception:
        pass
    return None


def fetch_relevant_news_page(keyword: str):
    """å„ªå…ˆï¼šWP REST -> Tavily -> None"""
    # è‹¥ keyword æœ¬èº«å°±æ˜¯ç¶²å€
    if keyword.startswith("http"):
        return keyword

    for kw in SYNONYMS.get(keyword.strip(), [keyword.strip()]):
        url = _search_wordpress_rest(kw)
        if url:
            return url
    # REST æ‰¾ä¸åˆ°å°±ç”¨ Tavily
    return _search_tavily(keyword)

# ====== ç›¸ä¼¼åº¦æŒ‘æ®µè½ ======

def retrieve_relevant_content(task: str, paragraphs: list[str]):
    if not paragraphs:
        return ""
    embeds = model.encode(paragraphs, convert_to_tensor=True)
    q_embed = model.encode(task, convert_to_tensor=True)
    sims = util.pytorch_cos_sim(q_embed, embeds)[0]
    top_k = min(10, len(paragraphs))
    best_idx = torch.topk(sims, k=top_k).indices
    return "\n".join([paragraphs[i] for i in best_idx])

# ====== è§£ææª”å ======

def get_filename_from_url(url: str):
    return unquote(os.path.basename(urlparse(url).path)).replace(" ", "_")

# ====== æ ¸å¿ƒé‚è¼¯ ======

def generate_response_combined(task: str, keyword: str):
    cleaned_paragraphs = []
    pdf_links: list[tuple[str, str]] = []

    page_url = fetch_relevant_news_page(keyword) if keyword.strip() else None

    # è®€å–å­é é¢ & å…§åµŒé™„ä»¶
    if page_url and page_url.startswith("http"):
        try:
            res = requests.get(page_url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            cleaned_paragraphs.extend(clean_and_split_text(soup.get_text()))

            pdf_urls = {
                urljoin(page_url, a["href"].replace(" ", "%20"))
                for a in soup.find_all("a", href=True)
                if a["href"].lower().endswith(".pdf")
            }
            for url in pdf_urls:
                try:
                    fname = get_filename_from_url(url)
                    local = os.path.join("downloads", fname)
                    with open(local, "wb") as f:
                        f.write(requests.get(url, timeout=10).content)
                    cleaned_paragraphs.extend(read_pdf(local))
                    pdf_links.append((fname, url))
                except Exception as e:
                    cleaned_paragraphs.append(f"âŒ ç„¡æ³•ä¸‹è¼‰é™„ä»¶ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")
        except Exception as e:
            cleaned_paragraphs.append(f"âŒ ç„¡æ³•è®€å–å­é é¢å…§å®¹ï¼š{e}")

    relevant = retrieve_relevant_content(task, cleaned_paragraphs)

    prompt = f"""
ä½ æ˜¯ä¸€ä½äº†è§£åŒ—ä¸€å¥³ä¸­è¡Œæ”¿æµç¨‹èˆ‡æ ¡å…§äº‹å‹™çš„è¼”å°è€å¸«ï¼Œè«‹æ ¹æ“šä¸‹æ–¹æä¾›çš„è³‡æ–™å”åŠ©å›ç­”å•é¡Œï¼Œ
è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä»¥æ¢åˆ—å¼æˆ–æ‘˜è¦æ–¹å¼ç°¡æ½”è¡¨é”ï¼Œå¦‚æœæœ‰äººè©¢å•åˆ°æ®µè€ƒç­‰é—œéµå­—ï¼Œå°‡å…¶è¦–ç‚ºæœŸä¸­è€ƒæˆ–æœŸæœ«è€ƒï¼Œä¸”å¯ä»¥å»æœå°‹è¡Œäº‹æ›†ã€‚è‹¥é—œéµå­—æ‰¾ä¸åˆ°è³‡è¨Šï¼Œä¸ç”¨ç®¡æª”æ¡ˆæ¨™é¡Œï¼Œç›´æ¥æ‰¾æœ‰æåˆ°æ­¤é—œéµå­—çš„æª”æ¡ˆï¼Œå¦‚æœé—œéµå­—æœå°‹ä¸åˆ°ï¼Œå°±æ”¹æˆç”¨ä½¿ç”¨è€…è¼¸å…¥çš„å•é¡Œå»æœå°‹ã€‚

å•é¡Œï¼š{task}

ç›¸é—œå…§å®¹ï¼š
{relevant if relevant else "ï¼ˆæœªæ‰¾åˆ°å…¶ä»–ç›¸é—œå…§å®¹ï¼‰"}

# å…§å»ºæ ¡åœ’å°çŸ¥è­˜ï¼ˆç•¥ï¼‰...
"""
    # çœç•¥ï¼šä¿ç•™åŸæœ¬çš„ Gemini API å‘¼å«ç¢¼
    # ... ï¼ˆæ­¤è™•èˆ‡ä¸Šä¸€ç‰ˆç›¸åŒï¼‰
    return "(ç•¥)"  # çœç•¥ï¼šä¿æŒå‡½å¼ç°½åå®Œæ•´

# ====== Streamlit ä»‹é¢ ======
st.title("ğŸŒ± ç¶ åœ’äº‹å‹™è©¢å•æ¬„")

task = st.text_input("è¼¸å…¥è©¢å•äº‹é …", "ä¾‹å¦‚ï¼šä»Šå¹´çš„ç•¢æ¥­å…¸ç¦®æ˜¯å“ªä¸€å¤©ï¼Ÿ")
keyword = st.text_input("è¼¸å…¥é—œéµå­—ï¼ˆå¾åŒ—ä¸€å¥³æ ¡ç¶²æœ€æ–°æ¶ˆæ¯ä¸­æœå°‹ï¼Œæˆ–ç›´æ¥è²¼ç¶²å€ï¼‰", "ä¾‹å¦‚ï¼šè¡Œäº‹æ›†")

if st.button("ç”Ÿæˆå›ç­”"):
    with st.spinner("æ­£åœ¨æœå°‹èˆ‡ç”Ÿæˆå›è¦†..."):
        st.markdown(generate_response_combined(task, keyword))

st.markdown("---")
# ï¼ˆä¸‹æ–¹æŒ‰éˆ•å€å¡Šä¸è®Šï¼‰
