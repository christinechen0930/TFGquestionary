import os
import re
import requests
import torch
import streamlit as st
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote, urlparse
from sentence_transformers import SentenceTransformer, util
import fitz  # PyMuPDF

# ====== è¨­å®š API Key ======

TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"] GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]

# ====== é é¢è¨­å®š ======

st.set_page_config(page_title="\U0001F33F ç¶ åœ’äº‹å‹™è©¢å•æ¬„", page_icon="\U0001F331", layout="centered") os.makedirs("downloads", exist_ok=True)

# ====== åŒç¾©è©å°ç…§è¡¨ ======

SYNONYMS = { "æ®µè€ƒ": ["æœŸä¸­è€ƒ", "æœŸæœ«è€ƒ"], "æœŸä¸­": ["æœŸä¸­è€ƒ"], "æœŸæœ«": ["æœŸæœ«è€ƒ"], "ç•¢æ¥­å…¸ç¦®": ["ç•¢å…¸", "ç•¢æ¥­å…¸ç¦®"], # å¯æ“´å……æ›´å¤š }

# ====== æ¨¡å‹åŠ è¼‰ ======

@st.cache_resource def load_model(): return SentenceTransformer("sentence-transformers/multi-qa-MiniLM-L6-cos-v1")

model = load_model()

# ====== æ¸…ç†æ–‡å­— ======

def clean_and_split_text(text): text = re.sub(r"\s+", " ", text) text = re.sub(r"ç¬¬\s*\d+\s*é ", "", text) paragraphs = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text) return [p.strip() for p in paragraphs if len(p.strip()) > 10]

# ====== è®€å– PDF ======

def read_pdf(file_path): try: doc = fitz.Document(file_path) all_paragraphs = [] for page in doc: raw_text = page.get_text() paragraphs = clean_and_split_text(raw_text) all_paragraphs.extend(paragraphs) return all_paragraphs except Exception as e: return [f"è®€å– PDF éŒ¯èª¤ï¼š{str(e)}"]

# ====== æ“·å–åŒ—ä¸€å¥³æœ€æ–°æ¶ˆæ¯ä¸­çš„é—œéµå­—å­é é¢ ======

def fetch_relevant_news_page(keyword): base_url = "https://www.fg.tp.edu.tw" news_url = f"{base_url}/category/news/news1/" try: res = requests.get(news_url, timeout=10) res.raise_for_status() except Exception: return None

soup = BeautifulSoup(res.text, "html.parser")
links = soup.find_all("a", href=True)

search_keywords = SYNONYMS.get(keyword.strip(), [keyword.strip()])

for link in links:
    title = link.get_text(strip=True)
    href = link["href"]
    if "/news/" in href:
        if any(kw in title for kw in search_keywords):
            return urljoin(base_url, href)

return None

# ====== æ‰¾åˆ°ç›¸é—œæ®µè½ ======

def retrieve_relevant_content(task, paragraphs): if not paragraphs: return "" paragraph_embeddings = model.encode(paragraphs, convert_to_tensor=True) query_embedding = model.encode(task, convert_to_tensor=True) scores = util.pytorch_cos_sim(query_embedding, paragraph_embeddings)[0] top_k = min(10, len(paragraphs)) top_results = torch.topk(scores, k=top_k) return "\n".join([paragraphs[idx] for idx in top_results.indices])

# ====== å¾ URL è§£æå‡ºåŸå§‹æª”å ======

def get_filename_from_url(url): path = urlparse(url).path return unquote(os.path.basename(path)).replace(" ", "_")

# ====== æ•´åˆå›ç­”é‚è¼¯ ======

def generate_response_combined(task, keyword): cleaned_paragraphs = [] pdf_links_collected = [] page_url = None

if keyword.strip():
    page_url = fetch_relevant_news_page(keyword)
    if page_url:
        try:
            res = requests.get(page_url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            content_text = soup.get_text()
            cleaned_paragraphs.extend(clean_and_split_text(content_text))

            pdf_links = {
                urljoin(page_url, a["href"].replace(" ", "%20"))
                for a in soup.find_all("a", href=True)
                if a["href"].endswith(".pdf")
            }

            for pdf_url in pdf_links:
                try:
                    file_name = get_filename_from_url(pdf_url)
                    local_path = os.path.join("downloads", file_name)
                    r = requests.get(pdf_url, timeout=10)
                    with open(local_path, "wb") as f:
                        f.write(r.content)
                    cleaned_paragraphs.extend(read_pdf(local_path))
                    pdf_links_collected.append((file_name, pdf_url))
                except Exception as e:
                    cleaned_paragraphs.append(f"âŒ ç„¡æ³•ä¸‹è¼‰é™„ä»¶ï¼š{pdf_url}ï¼ŒéŒ¯èª¤ï¼š{e}")
        except Exception as e:
            cleaned_paragraphs.append(f"âŒ ç„¡æ³•è®€å–å­é é¢å…§å®¹ï¼š{e}")

relevant_content = retrieve_relevant_content(task, cleaned_paragraphs)

prompt = f"""

ä½ æ˜¯ä¸€ä½äº†è§£åŒ—ä¸€å¥³ä¸­è¡Œæ”¿æµç¨‹èˆ‡æ ¡å…§äº‹å‹™çš„è¼”å°è€å¸«ï¼Œè«‹æ ¹æ“šä¸‹æ–¹æä¾›çš„è³‡æ–™å”åŠ©å›ç­”å•é¡Œï¼Œ è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä»¥æ¢åˆ—å¼æˆ–æ‘˜è¦æ–¹å¼ç°¡æ½”è¡¨é”ï¼Œå…è¨±ä½¿ç”¨ Markdown èªæ³•ï¼ˆç²—é«”ã€æ–œé«”ç­‰ï¼‰ï¼Œ å¦‚æœæœ‰äººè©¢å•åˆ°æ®µè€ƒç­‰é—œéµå­—ï¼Œå°‡å…¶è¦–ç‚ºæœŸä¸­è€ƒæˆ–æœŸæœ«è€ƒï¼Œä¸”å¯ä»¥å»æœå°‹è¡Œäº‹æ›†ã€‚ è‹¥é—œéµå­—æ‰¾ä¸åˆ°è³‡è¨Šï¼Œä¸ç”¨ç®¡æª”æ¡ˆæ¨™é¡Œï¼Œç›´æ¥æ‰¾æœ‰æåˆ°æ­¤é—œéµå­—çš„æª”æ¡ˆï¼Œ å¦‚æœé—œéµå­—æœå°‹ä¸åˆ°ï¼Œå°±æ”¹æˆç”¨ä½¿ç”¨è€…è¼¸å…¥çš„å•é¡Œå»æœå°‹ã€‚

å•é¡Œï¼š{task}

ç›¸é—œå…§å®¹ï¼š {relevant_content if relevant_content else "ï¼ˆæœªæ‰¾åˆ°å…¶ä»–ç›¸é—œå…§å®¹ï¼‰"}

é—œæ–¼åŒ—ä¸€å¥³ä¸­çš„å¤§å°äº‹ï¼š

åŒ—ä¸€å¥³ä¸­çš„åˆ¶æœï¼šç¶ è‰²ä¸Šè¡£é…é»‘è‰²ç™¾è¤¶è£™ã€‚é‹å‹•æœï¼šç™½è‰²ä¸Šè¡£é…é»‘è‰²è¤²å­ã€‚

è¬è–ç¯€å¯ä»¥ç©¿ä»»ä½•æœè£ï¼Œåªè¦ä½ æ•¢ç©¿ä¾†å°±æ²’äººæ””å¾—ä½ä½ ï¼Œåªæ˜¯è¦è¨˜å¾—å¸¶å­¸ç”Ÿè­‰ä»¥ä¾¿æ•™å®˜è¾¨èªèº«åˆ†ã€‚

åˆé¤é¸æ“‡ï¼šä¸èƒ½å‡ºæ ¡åƒåˆé¤ï¼Œå¯è‡³å¤§å°ç†±è³¼è²·ã€è¨‚å¤–é€ã€æˆ–è‡ªå‚™ä¾¿ç•¶ã€‚

å°ç†±è²©å”®ï¼šé›¶é£Ÿã€é¤…ä¹¾ã€ç‚¸ç‰©ã€åœŸå¸ï¼Œä¾‹å¦‚è–¯ä¸è¾£ï¼ˆè–¯æ¢+ç”œä¸è¾£ï¼‰ã€å·§å…‹åŠ›å’”å•¦é›ï¼ˆå·§å…‹åŠ›åå¸+å’”å•¦é›ï¼‰ç­‰è‡ªç”±æ­é…ã€‚

å¤§ç†±è²©å”®ï¼šæ»·è‚‰é£¯ã€ç‚’é£¯ã€é‹ç‡’éºµã€ä¾¿ç•¶ï¼Œå¶æœ‰ç³–è‘«è˜†ã€ä»™è‰èœœã€èŠ’æœå†°ç­‰ç‰¹åˆ¥å“é …ã€‚

å…‰å¾©æ¨“ç‚ºå¤è¹Ÿã€‚

å­¸ç æ¨“æ˜¯ä»¥æ±Ÿå­¸ç æ ¡é•·çš„åå­—å‘½åã€‚

æ•™å­¸æ¨“åˆ†å¸ƒï¼š

å…‰å¾©æ¨“ï¼š1F å­¸å‹™/æ•™å‹™/æ ¡å®‰/å¥åº·ä¸­å¿ƒï¼›2-3F é«˜ä¸€æ•™å®¤

å­¸ç æ¨“ï¼š2-3F åœ–æ›¸é¤¨ï¼›4F è€å¸«è¾¦å…¬å®¤ï¼›5F é›»è…¦æ•™å®¤ï¼›6F æ¼”è¬›å»³

ä¸­æ­£æ¨“ï¼š1-3F é«˜äºŒæ•™å®¤

è‡³å–„æ¨“ï¼š1F è¼”å°å®¤ï¼›2-5F é«˜ä¸‰æ•™å®¤ï¼›3-4F å¯¦é©—å®¤ï¼›5F ç¾è¡“æ•™å®¤

æ˜å¾·æ¨“ï¼š1F ç”Ÿç§‘æ•™å®¤ï¼›2-3F éŸ³æ¨‚æ•™å®¤ """


api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent" headers = {"Content-Type": "application/json"} payload = { "contents": [{"role": "user", "parts": [{"text": prompt}]}] }

try: response = requests.post(f"{api_url}?key={GEMINI_API_KEY}", json=payload, headers=headers) if response.status_code == 200: response_json = response.json() if "candidates" in response_json and len(response_json["candidates"]) > 0: model_reply = response_json["candidates"][0]["content"]["parts"][0]["text"] attachments_text = "" if pdf_links_collected: attachments_text += "\n\U0001F4CE é™„ä»¶ä¸‹è¼‰ï¼š\n" for name, link in pdf_links_collected: attachments_text += f"- {name}\n" source_note = f"\n\n---\n\U0001F517 ä¾†æºå­é é¢" if page_url else "\n\n---\nâš ï¸ æœªå¾æ ¡ç¶²æ‰¾åˆ°å­é é¢ã€‚" return model_reply + source_note + attachments_text else: return "âŒ ç„¡æ³•å–å¾—æ¨¡å‹å›ç­”" else: return f"âŒ éŒ¯èª¤ï¼š{response.status_code}, {response.text}" except Exception as e: return f"âŒ è«‹æ±‚å¤±æ•—ï¼š{e}"


# ====== Streamlit ä»‹é¢ ======

st.title("\U0001F331 ç¶ åœ’äº‹å‹™è©¢å•æ¬„")

task = st.text_input("è¼¸å…¥è©¢å•äº‹é …", "ä¾‹å¦‚ï¼šä»Šå¹´çš„ç•¢æ¥­å…¸ç¦®æ˜¯å“ªä¸€å¤©ï¼Ÿ") keyword = st.text_input("è¼¸å…¥é—œéµå­—ï¼ˆå¾åŒ—ä¸€å¥³æ ¡ç¶²æœ€æ–°æ¶ˆæ¯ä¸­æœå°‹ï¼‰", "ä¾‹å¦‚ï¼šç•¢æ¥­å…¸ç¦®")

if st.button("ç”Ÿæˆå›ç­”"): with st.spinner('æ­£åœ¨æœå°‹èˆ‡ç”Ÿæˆå›è¦†...'): response = generate_response_combined(task, keyword) st.success('è™•ç†å®Œæˆï¼') st.markdown(response, unsafe_allow_html=True)

st.markdown("---")

st.markdown("""

<style>
.button-container {
    display: flex;
    justify-content: flex-start;
    gap: 0.5rem;
    margin-top: 1rem;
}

a.fake-button {
    background-color: #f0f2f6;
    color: black;
    padding: 0.4rem 1rem;
    text-decoration: none;
    border-radius: 0.5rem;
    border: 1px solid #d3d3d3;
    font-weight: 500;
    font-size: 1rem;
    display: inline-block;
    transition: background-color 0.2s ease;
}

a.fake-button:hover {
    background-color: #e0e0e0;
}
</style><div class="button-container">
    <a href="https://christinechen0930.github.io/TFGquestionary/TFGQA.html" target="_blank" class="fake-button">ğŸ” å‰å¾€åŒ—ä¸€å¥³ä¸­å•ç­”é›†</a>
    <a href="https://christinechen0930.github.io/TFGquestionary/TFGhistory.html" target="_blank" class="fake-button">ğŸ“œ ç­è§£åŒ—ä¸€å¥³æ ¡å²</a>
</div>
""", unsafe_allow_html=True)import os import re import requests import torch import streamlit as st from bs4 import BeautifulSoup from urllib.parse import urljoin, unquote, urlparse from sentence_transformers import SentenceTransformer, util import fitz  # PyMuPDF

# ====== è¨­å®š API Key ======

TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]

# ====== é é¢è¨­å®š ======

st.set_page_config(page_title="\U0001F33F ç¶ åœ’äº‹å‹™è©¢å•æ¬„", page_icon="\U0001F331", layout="centered") os.makedirs("downloads", exist_ok=True)

# ====== åŒç¾©è©å°ç…§è¡¨ ======

SYNONYMS = { "æ®µè€ƒ": ["æœŸä¸­è€ƒ", "æœŸæœ«è€ƒ"], "æœŸä¸­": ["æœŸä¸­è€ƒ"], "æœŸæœ«": ["æœŸæœ«è€ƒ"], "ç•¢æ¥­å…¸ç¦®": ["ç•¢å…¸", "ç•¢æ¥­å…¸ç¦®"], # å¯æ“´å……æ›´å¤š }

# ====== æ¨¡å‹åŠ è¼‰ ======

@st.cache_resource def load_model(): return SentenceTransformer("sentence-transformers/multi-qa-MiniLM-L6-cos-v1")

model = load_model()

# ====== æ¸…ç†æ–‡å­— ======

def clean_and_split_text(text): text = re.sub(r"\s+", " ", text) text = re.sub(r"ç¬¬\s*\d+\s*é ", "", text) paragraphs = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text) return [p.strip() for p in paragraphs if len(p.strip()) > 10]

# ====== è®€å– PDF ======

def read_pdf(file_path): try: doc = fitz.Document(file_path) all_paragraphs = [] for page in doc: raw_text = page.get_text() paragraphs = clean_and_split_text(raw_text) all_paragraphs.extend(paragraphs) return all_paragraphs except Exception as e: return [f"è®€å– PDF éŒ¯èª¤ï¼š{str(e)}"]

# ====== æ“·å–åŒ—ä¸€å¥³æœ€æ–°æ¶ˆæ¯ä¸­çš„é—œéµå­—å­é é¢ ======

def fetch_relevant_news_page(keyword): base_url = "https://www.fg.tp.edu.tw" news_url = f"{base_url}/category/news/news1/" try: res = requests.get(news_url, timeout=10) res.raise_for_status() except Exception: return None

soup = BeautifulSoup(res.text, "html.parser")
links = soup.find_all("a", href=True)

search_keywords = SYNONYMS.get(keyword.strip(), [keyword.strip()])

for link in links:
    title = link.get_text(strip=True)
    href = link["href"]
    if "/news/" in href:
        if any(kw in title for kw in search_keywords):
            return urljoin(base_url, href)

return None

# ====== æ‰¾åˆ°ç›¸é—œæ®µè½ ======

def retrieve_relevant_content(task, paragraphs): if not paragraphs: return "" paragraph_embeddings = model.encode(paragraphs, convert_to_tensor=True) query_embedding = model.encode(task, convert_to_tensor=True) scores = util.pytorch_cos_sim(query_embedding, paragraph_embeddings)[0] top_k = min(10, len(paragraphs)) top_results = torch.topk(scores, k=top_k) return "\n".join([paragraphs[idx] for idx in top_results.indices])

# ====== å¾ URL è§£æå‡ºåŸå§‹æª”å ======

def get_filename_from_url(url): path = urlparse(url).path return unquote(os.path.basename(path)).replace(" ", "_")

# ====== æ•´åˆå›ç­”é‚è¼¯ ======

def generate_response_combined(task, keyword): cleaned_paragraphs = [] pdf_links_collected = [] page_url = None

if keyword.strip():
    page_url = fetch_relevant_news_page(keyword)
    if page_url:
        try:
            res = requests.get(page_url, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            content_text = soup.get_text()
            cleaned_paragraphs.extend(clean_and_split_text(content_text))

            pdf_links = {
                urljoin(page_url, a["href"].replace(" ", "%20"))
                for a in soup.find_all("a", href=True)
                if a["href"].endswith(".pdf")
            }

            for pdf_url in pdf_links:
                try:
                    file_name = get_filename_from_url(pdf_url)
                    local_path = os.path.join("downloads", file_name)
                    r = requests.get(pdf_url, timeout=10)
                    with open(local_path, "wb") as f:
                        f.write(r.content)
                    cleaned_paragraphs.extend(read_pdf(local_path))
                    pdf_links_collected.append((file_name, pdf_url))
                except Exception as e:
                    cleaned_paragraphs.append(f"âŒ ç„¡æ³•ä¸‹è¼‰é™„ä»¶ï¼š{pdf_url}ï¼ŒéŒ¯èª¤ï¼š{e}")
        except Exception as e:
            cleaned_paragraphs.append(f"âŒ ç„¡æ³•è®€å–å­é é¢å…§å®¹ï¼š{e}")

relevant_content = retrieve_relevant_content(task, cleaned_paragraphs)

prompt = f"""

ä½ æ˜¯ä¸€ä½äº†è§£åŒ—ä¸€å¥³ä¸­è¡Œæ”¿æµç¨‹èˆ‡æ ¡å…§äº‹å‹™çš„è¼”å°è€å¸«ï¼Œè«‹æ ¹æ“šä¸‹æ–¹æä¾›çš„è³‡æ–™å”åŠ©å›ç­”å•é¡Œï¼Œ è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä»¥æ¢åˆ—å¼æˆ–æ‘˜è¦æ–¹å¼ç°¡æ½”è¡¨é”ï¼Œå…è¨±ä½¿ç”¨ Markdown èªæ³•ï¼ˆç²—é«”ã€æ–œé«”ç­‰ï¼‰ï¼Œ å¦‚æœæœ‰äººè©¢å•åˆ°æ®µè€ƒç­‰é—œéµå­—ï¼Œå°‡å…¶è¦–ç‚ºæœŸä¸­è€ƒæˆ–æœŸæœ«è€ƒï¼Œä¸”å¯ä»¥å»æœå°‹è¡Œäº‹æ›†ã€‚ è‹¥é—œéµå­—æ‰¾ä¸åˆ°è³‡è¨Šï¼Œä¸ç”¨ç®¡æª”æ¡ˆæ¨™é¡Œï¼Œç›´æ¥æ‰¾æœ‰æåˆ°æ­¤é—œéµå­—çš„æª”æ¡ˆï¼Œ å¦‚æœé—œéµå­—æœå°‹ä¸åˆ°ï¼Œå°±æ”¹æˆç”¨ä½¿ç”¨è€…è¼¸å…¥çš„å•é¡Œå»æœå°‹ã€‚

å•é¡Œï¼š{task}

ç›¸é—œå…§å®¹ï¼š {relevant_content if relevant_content else "ï¼ˆæœªæ‰¾åˆ°å…¶ä»–ç›¸é—œå…§å®¹ï¼‰"}

é—œæ–¼åŒ—ä¸€å¥³ä¸­çš„å¤§å°äº‹ï¼š

åŒ—ä¸€å¥³ä¸­çš„åˆ¶æœï¼šç¶ è‰²ä¸Šè¡£é…é»‘è‰²ç™¾è¤¶è£™ã€‚é‹å‹•æœï¼šç™½è‰²ä¸Šè¡£é…é»‘è‰²è¤²å­ã€‚

è¬è–ç¯€å¯ä»¥ç©¿ä»»ä½•æœè£ï¼Œåªè¦ä½ æ•¢ç©¿ä¾†å°±æ²’äººæ””å¾—ä½ä½ ï¼Œåªæ˜¯è¦è¨˜å¾—å¸¶å­¸ç”Ÿè­‰ä»¥ä¾¿æ•™å®˜è¾¨èªèº«åˆ†ã€‚

åˆé¤é¸æ“‡ï¼šä¸èƒ½å‡ºæ ¡åƒåˆé¤ï¼Œå¯è‡³å¤§å°ç†±è³¼è²·ã€è¨‚å¤–é€ã€æˆ–è‡ªå‚™ä¾¿ç•¶ã€‚

å°ç†±è²©å”®ï¼šé›¶é£Ÿã€é¤…ä¹¾ã€ç‚¸ç‰©ã€åœŸå¸ï¼Œä¾‹å¦‚è–¯ä¸è¾£ï¼ˆè–¯æ¢+ç”œä¸è¾£ï¼‰ã€å·§å…‹åŠ›å’”å•¦é›ï¼ˆå·§å…‹åŠ›åå¸+å’”å•¦é›ï¼‰ç­‰è‡ªç”±æ­é…ã€‚

å¤§ç†±è²©å”®ï¼šæ»·è‚‰é£¯ã€ç‚’é£¯ã€é‹ç‡’éºµã€ä¾¿ç•¶ï¼Œå¶æœ‰ç³–è‘«è˜†ã€ä»™è‰èœœã€èŠ’æœå†°ç­‰ç‰¹åˆ¥å“é …ã€‚

å…‰å¾©æ¨“ç‚ºå¤è¹Ÿã€‚

å­¸ç æ¨“æ˜¯ä»¥æ±Ÿå­¸ç æ ¡é•·çš„åå­—å‘½åã€‚

æ•™å­¸æ¨“åˆ†å¸ƒï¼š

å…‰å¾©æ¨“ï¼š1F å­¸å‹™/æ•™å‹™/æ ¡å®‰/å¥åº·ä¸­å¿ƒï¼›2-3F é«˜ä¸€æ•™å®¤

å­¸ç æ¨“ï¼š2-3F åœ–æ›¸é¤¨ï¼›4F è€å¸«è¾¦å…¬å®¤ï¼›5F é›»è…¦æ•™å®¤ï¼›6F æ¼”è¬›å»³

ä¸­æ­£æ¨“ï¼š1-3F é«˜äºŒæ•™å®¤

è‡³å–„æ¨“ï¼š1F è¼”å°å®¤ï¼›2-5F é«˜ä¸‰æ•™å®¤ï¼›3-4F å¯¦é©—å®¤ï¼›5F ç¾è¡“æ•™å®¤

æ˜å¾·æ¨“ï¼š1F ç”Ÿç§‘æ•™å®¤ï¼›2-3F éŸ³æ¨‚æ•™å®¤ """


api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent" headers = {"Content-Type": "application/json"} payload = { "contents": [{"role": "user", "parts": [{"text": prompt}]}] }

try: response = requests.post(f"{api_url}?key={GEMINI_API_KEY}", json=payload, headers=headers) if response.status_code == 200: response_json = response.json() if "candidates" in response_json and len(response_json["candidates"]) > 0: model_reply = response_json["candidates"][0]["content"]["parts"][0]["text"] attachments_text = "" if pdf_links_collected: attachments_text += "\n\U0001F4CE é™„ä»¶ä¸‹è¼‰ï¼š\n" for name, link in pdf_links_collected: attachments_text += f"- {name}\n" source_note = f"\n\n---\n\U0001F517 ä¾†æºå­é é¢" if page_url else "\n\n---\nâš ï¸ æœªå¾æ ¡ç¶²æ‰¾åˆ°å­é é¢ã€‚" return model_reply + source_note + attachments_text else: return "âŒ ç„¡æ³•å–å¾—æ¨¡å‹å›ç­”" else: return f"âŒ éŒ¯èª¤ï¼š{response.status_code}, {response.text}" except Exception as e: return f"âŒ è«‹æ±‚å¤±æ•—ï¼š{e}"


====== Streamlit ä»‹é¢ ======

st.title("\U0001F331 ç¶ åœ’äº‹å‹™è©¢å•æ¬„")

task = st.text_input("è¼¸å…¥è©¢å•äº‹é …", "ä¾‹å¦‚ï¼šä»Šå¹´çš„ç•¢æ¥­å…¸ç¦®æ˜¯å“ªä¸€å¤©ï¼Ÿ") keyword = st.text_input("è¼¸å…¥é—œéµå­—ï¼ˆå¾åŒ—ä¸€å¥³æ ¡ç¶²æœ€æ–°æ¶ˆæ¯ä¸­æœå°‹ï¼‰", "ä¾‹å¦‚ï¼šç•¢æ¥­å…¸ç¦®")

if st.button("ç”Ÿæˆå›ç­”"): with st.spinner('æ­£åœ¨æœå°‹èˆ‡ç”Ÿæˆå›è¦†...'): response = generate_response_combined(task, keyword) st.success('è™•ç†å®Œæˆï¼') st.markdown(response, unsafe_allow_html=True)

st.markdown("---")

st.markdown("""

<style>
.button-container {
    display: flex;
    justify-content: flex-start;
    gap: 0.5rem;
    margin-top: 1rem;
}

a.fake-button {
    background-color: #f0f2f6;
    color: black;
    padding: 0.4rem 1rem;
    text-decoration: none;
    border-radius: 0.5rem;
    border: 1px solid #d3d3d3;
    font-weight: 500;
    font-size: 1rem;
    display: inline-block;
    transition: background-color 0.2s ease;
}

a.fake-button:hover {
    background-color: #e0e0e0;
}
</style><div class="button-container">
    <a href="https://christinechen0930.github.io/TFGquestionary/TFGQA.html" target="_blank" class="fake-button">ğŸ” å‰å¾€åŒ—ä¸€å¥³ä¸­å•ç­”é›†</a>
    <a href="https://christinechen0930.github.io/TFGquestionary/TFGhistory.html" target="_blank" class="fake-button">ğŸ“œ ç­è§£åŒ—ä¸€å¥³æ ¡å²</a>
</div>
""", unsafe_allow_html=True)
