import os
import re
import requests
import torch
import streamlit as st
from sentence_transformers import SentenceTransformer, util
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import fitz  # PyMuPDF
from tavily import TavilyClient

# ====== è¨­å®š API Key ======
TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)

# ====== é é¢è¨­å®š ======
st.set_page_config(page_title="ğŸŒ¿ ç¶ åœ’äº‹å‹™è©¢å•æ¬„", page_icon="ğŸŒ±", layout="centered")
os.makedirs("downloads", exist_ok=True)

# ====== æ¨¡å‹åŠ è¼‰ ======
@st.cache_resource
def load_model():
    return SentenceTransformer("sentence-transformers/multi-qa-MiniLM-L6-cos-v1")

model = load_model()

# ====== æœå°‹ä¸¦ä¸‹è¼‰æœ€æ–° PDF æˆ–æ“·å–å­é å…§å®¹ ======
def search_and_process_content(keyword):
    query = f"site:fg.tp.edu.tw {keyword}"
    try:
        response = tavily_client.search(
            query,
            search_depth="advanced",
            max_results=5,
            sort_by="date"
        )
    except Exception as e:
        return f"âŒ æœå°‹æœå‹™éŒ¯èª¤ï¼š{e}"

    results = response.get("results", [])
    subpages = [r for r in results if not urlparse(r['url']).netloc.endswith("fg.tp.edu.tw") or "/news/" in r["url"]]

    if not subpages:
        suggest_words = ["æ‹›ç”Ÿ", "æ ¡å…§å…¬å‘Š", "å­¸ç”Ÿæ´»å‹•", "æ ¡è¦", "äº¤æ›å­¸ç”Ÿ"]
        suggestion = suggest_words[torch.randint(0, len(suggest_words), (1,)).item()]
        return f"âŒ æ‰¾ä¸åˆ°ç¬¦åˆã€Œ{keyword}ã€çš„å­é é¢ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ï¼Œä¾‹å¦‚ï¼š**{suggestion}**"

    top_url = subpages[0]['url']
    try:
        page_resp = requests.get(top_url, timeout=10)
        soup = BeautifulSoup(page_resp.text, 'html.parser')
        page_text = soup.get_text()
        page_text = re.sub(r"\s+", " ", page_text).strip()

        pdf_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.pdf')]
        pdf_info = None
        if pdf_links:
            pdf_url = pdf_links[0] if pdf_links[0].startswith("http") else top_url.rsplit('/', 1)[0] + '/' + pdf_links[0]
            pdf_response = requests.get(pdf_url)
            safe_keyword = re.sub(r'[\\/*?:"<>|]', "_", keyword)
            pdf_filename = os.path.join("downloads", f"{safe_keyword}_attached.pdf")
            with open(pdf_filename, "wb") as f:
                f.write(pdf_response.content)
            pdf_info = {"path": pdf_filename, "url": pdf_url}

        return {"url": top_url, "text": page_text, "pdf": pdf_info}

    except Exception as e:
        return f"âŒ ç„¡æ³•æ“·å–å­é é¢å…§å®¹ï¼š{e}"

# ====== æ¸…ç†æ–‡å­— ======
def clean_and_split_text(text):
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"ç¬¬\s*\d+\s*é ", "", text)
    paragraphs = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text)
    return [p.strip() for p in paragraphs if len(p.strip()) > 10]

# ====== è®€å– PDF ======
def read_pdf(file_path):
    try:
        doc = fitz.Document(file_path)
        all_paragraphs = []
        for page in doc:
            raw_text = page.get_text()
            paragraphs = clean_and_split_text(raw_text)
            all_paragraphs.extend(paragraphs)
        return all_paragraphs
    except Exception as e:
        return [f"è®€å– PDF éŒ¯èª¤ï¼š{str(e)}"]

# ====== æ‰¾åˆ°ç›¸é—œæ®µè½ ======
def retrieve_relevant_content(task, paragraphs):
    paragraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)
    query_embedding = model.encode(task, convert_to_tensor=True)
    scores = util.pytorch_cos_sim(query_embedding, paragraph_embeddings)[0]
    top_k = min(10, len(paragraphs))
    top_results = torch.topk(scores, k=top_k)
    return "\n".join([paragraphs[idx] for idx in top_results.indices])

# ====== æ•´åˆå›ç­” ======
def generate_response_combined(task, keyword):
    if not keyword.strip():
        return "âŒ è«‹è¼¸å…¥é—œéµå­—"

    result = search_and_process_content(keyword)
    if isinstance(result, str):
        return result

    paragraphs = clean_and_split_text(result["text"])

    if result.get("pdf"):
        pdf_paragraphs = read_pdf(result["pdf"]["path"])
        if pdf_paragraphs and "éŒ¯èª¤" not in pdf_paragraphs[0]:
            paragraphs.extend(pdf_paragraphs)

    if not paragraphs:
        return "âŒ ç„¡æ³•æ“·å–ä»»ä½•æ–‡å­—å…§å®¹ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚"

    relevant_content = retrieve_relevant_content(task, paragraphs)
    if not relevant_content.strip():
        return "âŒ æ‰¾ä¸åˆ°èˆ‡å•é¡Œç›¸é—œçš„å…§å®¹ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚"

    source_links = f"- [ä¾†æºç¶²é ]({result['url']})"
    if result.get("pdf"):
        source_links += f"\n- [é™„åŠ PDF]({result['pdf']['url']})"

    prompt = f"""
ä½ æ˜¯ä¸€ä½äº†è§£åŒ—ä¸€å¥³ä¸­è¡Œæ”¿æµç¨‹èˆ‡æ ¡å…§äº‹å‹™çš„è¼”å°è€å¸«ï¼Œè«‹æ ¹æ“šä¸‹æ–¹æä¾›çš„å…§å®¹å”åŠ©å›ç­”å•é¡Œï¼Œ
è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä»¥æ¢åˆ—å¼æˆ–æ‘˜è¦æ–¹å¼ç°¡æ½”è¡¨é”ã€‚

å•é¡Œï¼š{task}

ç›¸é—œå…§å®¹ï¼š
{relevant_content}

ä¾†æºæ¸…å–®ï¼š
{source_links}
"""

    api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [
            {"role": "user", "parts": [{"text": prompt}]}
        ]
    }

    try:
        response = requests.post(f"{api_url}?key={GEMINI_API_KEY}", json=payload, headers=headers)
        if response.status_code == 200:
            response_json = response.json()
            if "candidates" in response_json and len(response_json["candidates"]) > 0:
                model_reply = response_json["candidates"][0]["content"]["parts"][0]["text"]
                return model_reply + "\n\n---\n### ğŸ“„ è³‡æ–™ä¾†æº\n" + source_links
            else:
                return "âŒ ç„¡æ³•å–å¾—æ¨¡å‹å›ç­”"
        else:
            return f"âŒ éŒ¯èª¤ï¼š{response.status_code}, {response.text}"
    except Exception as e:
        return f"âŒ è«‹æ±‚å¤±æ•—ï¼š{e}"

# ====== Streamlit ä»‹é¢ ======
st.title("ğŸŒ± ç¶ åœ’äº‹å‹™è©¢å•æ¬„")

task = st.text_input("è¼¸å…¥è©¢å•äº‹é …", "ä¾‹å¦‚ï¼šç•¢æ¥­å…¸ç¦®æµç¨‹ï¼Ÿ")
keyword = st.text_input("è¼¸å…¥é—œéµå­—ï¼ˆè‡ªå‹•æœå°‹åŒ—ä¸€å¥³ç¶²ç«™ï¼‰", "ä¾‹å¦‚ï¼šç•¢æ¥­å…¸ç¦®")

if st.button("ç”Ÿæˆå›ç­”"):
    with st.spinner('æ­£åœ¨è™•ç†...'):
        response = generate_response_combined(task, keyword)
        st.success('è™•ç†å®Œæˆï¼')
        st.markdown(response)

st.markdown("---")
if st.button("ç­è§£åŒ—ä¸€å¥³æ ¡å²"):
    js = "window.open('https://christinechen0930.github.io/TFGquestionary/TFGhistory.html')"
    st.components.v1.html(f"<script>{js}</script>", height=0, width=0)
