import os
import re
import requests
import torch
import streamlit as st
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote, urlparse
from sentence_transformers import SentenceTransformer, util
import fitz  # PyMuPDF

# ====== è¨­å®š API Key ======
TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]

# ====== é é¢è¨­å®š ======
st.set_page_config(page_title="ğŸŒ¿ ç¶ åœ’äº‹å‹™è©¢å•æ¬„", page_icon="ğŸŒ±", layout="centered")
os.makedirs("downloads", exist_ok=True)

# ====== æ¨¡å‹åŠ è¼‰ ======
@st.cache_resource
def load_model():
    return SentenceTransformer("sentence-transformers/multi-qa-MiniLM-L6-cos-v1")

model = load_model()

# ====== æ¸…ç†æ–‡å­— ======
def clean_and_split_text(text):
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"ç¬¬\s*\d+\s*é ", "", text)
    paragraphs = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', text)
    return [p.strip() for p in paragraphs if len(p.strip()) > 10]

# ====== è®€å– PDF ======
def read_pdf(file_path):
    try:
        doc = fitz.Document(file_path)
        all_paragraphs = []
        for page in doc:
            raw_text = page.get_text()
            paragraphs = clean_and_split_text(raw_text)
            all_paragraphs.extend(paragraphs)
        return all_paragraphs
    except Exception as e:
        return [f"è®€å– PDF éŒ¯èª¤ï¼š{str(e)}"]

# ====== æ“·å–åŒ—ä¸€å¥³æœ€æ–°æ¶ˆæ¯ä¸­çš„é—œéµå­—å­é é¢ ======
def fetch_relevant_news_page(keyword):
    base_url = "https://www.fg.tp.edu.tw"
    news_url = f"{base_url}/category/news/news1/"
    try:
        res = requests.get(news_url, timeout=10)
        res.raise_for_status()
    except Exception:
        return None  # â— æ”¹ç‚ºå›å‚³ Noneï¼Œä¸çµ‚æ­¢æµç¨‹

    soup = BeautifulSoup(res.text, "html.parser")
    links = soup.find_all("a", href=True)

    for link in links:
        title = link.get_text(strip=True)
        href = link["href"]
        if keyword in title and "/news/" in href:
            return urljoin(base_url, href)

    return None  # â— æ²’æ‰¾åˆ°å°±å› None

# ====== æ‰¾åˆ°ç›¸é—œæ®µè½ ======
def retrieve_relevant_content(task, paragraphs):
    if not paragraphs:
        return ""
    paragraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)
    query_embedding = model.encode(task, convert_to_tensor=True)
    scores = util.pytorch_cos_sim(query_embedding, paragraph_embeddings)[0]
    top_k = min(10, len(paragraphs))
    top_results = torch.topk(scores, k=top_k)
    return "\n".join([paragraphs[idx] for idx in top_results.indices])

# ====== å¾ URL è§£æå‡ºåŸå§‹æª”å ======
def get_filename_from_url(url):
    path = urlparse(url).path
    return unquote(os.path.basename(path)).replace(" ", "_")

# ====== æ•´åˆå›ç­”é‚è¼¯ ======
def generate_response_combined(task, keyword):
    cleaned_paragraphs = []
    pdf_links_collected = []
    page_url = None

    if keyword.strip():
        page_url = fetch_relevant_news_page(keyword)
        if page_url:
            try:
                res = requests.get(page_url, timeout=10)
                res.raise_for_status()
                soup = BeautifulSoup(res.text, "html.parser")
                content_text = soup.get_text()
                cleaned_paragraphs.extend(clean_and_split_text(content_text))

                # æ“·å– PDF
                pdf_links = {
                    urljoin(page_url, a["href"].replace(" ", "%20"))
                    for a in soup.find_all("a", href=True)
                    if a["href"].endswith(".pdf")
                }

                for pdf_url in pdf_links:
                    try:
                        file_name = get_filename_from_url(pdf_url)
                        local_path = os.path.join("downloads", file_name)
                        r = requests.get(pdf_url, timeout=10)
                        with open(local_path, "wb") as f:
                            f.write(r.content)
                        cleaned_paragraphs.extend(read_pdf(local_path))
                        pdf_links_collected.append((file_name, pdf_url))
                    except Exception as e:
                        cleaned_paragraphs.append(f"âŒ ç„¡æ³•ä¸‹è¼‰é™„ä»¶ï¼š{pdf_url}ï¼ŒéŒ¯èª¤ï¼š{e}")
            except Exception as e:
                cleaned_paragraphs.append(f"âŒ ç„¡æ³•è®€å–å­é é¢å…§å®¹ï¼š{e}")

    # ğŸ” ä¸ç®¡æœ‰æ²’æœ‰æ‰¾åˆ°ç¶²é ï¼Œéƒ½è¦ç¹¼çºŒè™•ç†
    relevant_content = retrieve_relevant_content(task, cleaned_paragraphs)

    prompt = f"""
ä½ æ˜¯ä¸€ä½äº†è§£åŒ—ä¸€å¥³ä¸­è¡Œæ”¿æµç¨‹èˆ‡æ ¡å…§äº‹å‹™çš„è¼”å°è€å¸«ï¼Œè«‹æ ¹æ“šä¸‹æ–¹æä¾›çš„è³‡æ–™å”åŠ©å›ç­”å•é¡Œï¼Œ
è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œä»¥æ¢åˆ—å¼æˆ–æ‘˜è¦æ–¹å¼ç°¡æ½”è¡¨é”ã€‚

å•é¡Œï¼š{task}

ç›¸é—œå…§å®¹ï¼š
{relevant_content if relevant_content else "ï¼ˆæœªæ‰¾åˆ°å…¶ä»–ç›¸é—œå…§å®¹ï¼‰"}

é—œæ–¼åŒ—ä¸€å¥³ä¸­çš„å¤§å°äº‹ï¼š
1. åŒ—ä¸€å¥³ä¸­åˆ¶æœä¸Šè¡£æ˜¯ç¶ è‰²çš„ï¼Œé‹å‹•æœä¸Šè¡£æ˜¯ç™½è‰²çš„ã€‚
2. è¬è–ç¯€å¯ä»¥ç©¿ä»»ä½•æœè£ï¼ŒåŒ…æ‹¬å‹æ ¡åˆ¶æœã€å—ç“œè£ã€åŒ—ä¸€åˆ¶æœã€‚
3. åŒ—ä¸€å¥³ä¸­å­¸ç”Ÿä¸æœƒå‡ºæ ¡åƒåˆé¤ï¼Œæœƒåƒå¤§å°ç†±çš„é£Ÿç‰©ï¼Œæˆ–è€…è¨‚å¤–é€ã€è‡ªå·±å¸¶ä¾¿ç•¶ã€‚
4. å°ç†±çš„è–¯ä¸è¾£æ˜¯ç”±è–¯æ¢å’Œç”œä¸è¾£æ­é…è€Œæˆã€‚
5. å¤§ç†±è¿‘å…©å¹´æ²’æœ‰è³£éé¹½é…¥é›ï¼Œæœ‰è³£éç³–è‘«è˜†ã€ä»™è‰èœœã€èŠ’æœå†°ã€‚
6. åŒ—ä¸€å¥³ä¸­å…‰å¾©æ¨“æ˜¯å¤è¹Ÿã€‚
7. åŒ—ä¸€å¥³ä¸­å­¸ç æ¨“æ˜¯ä»¥æ±Ÿå­¸ç æ ¡é•·çš„åå­—å‘½åã€‚
8. ä»æ„›æ¨“ä¸åœ¨åŒ—ä¸€å¥³ä¸­ã€‚
"""

    api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"role": "user", "parts": [{"text": prompt}]}]
    }

    try:
        response = requests.post(f"{api_url}?key={GEMINI_API_KEY}", json=payload, headers=headers)
        if response.status_code == 200:
            response_json = response.json()
            if "candidates" in response_json and len(response_json["candidates"]) > 0:
                model_reply = response_json["candidates"][0]["content"]["parts"][0]["text"]
                attachments_text = ""
                if pdf_links_collected:
                    attachments_text += "\nğŸ“ é™„ä»¶ä¸‹è¼‰ï¼š\n"
                    for name, link in pdf_links_collected:
                        attachments_text += f"- [{name}]({link})\n"
                source_note = f"\n\n---\nğŸ”— [ä¾†æºå­é é¢]({page_url})" if page_url else "\n\n---\nâš ï¸ æœªå¾æ ¡ç¶²æ‰¾åˆ°å­é é¢ã€‚"
                return model_reply + source_note + attachments_text
            else:
                return "âŒ ç„¡æ³•å–å¾—æ¨¡å‹å›ç­”"
        else:
            return f"âŒ éŒ¯èª¤ï¼š{response.status_code}, {response.text}"
    except Exception as e:
        return f"âŒ è«‹æ±‚å¤±æ•—ï¼š{e}"

# ====== Streamlit ä»‹é¢ ======
st.title("ğŸŒ± ç¶ åœ’äº‹å‹™è©¢å•æ¬„")

task = st.text_input("è¼¸å…¥è©¢å•äº‹é …", "ä¾‹å¦‚ï¼šä»Šå¹´çš„ç•¢æ¥­å…¸ç¦®æ˜¯å“ªä¸€å¤©ï¼Ÿ")
keyword = st.text_input("è¼¸å…¥é—œéµå­—ï¼ˆå¾åŒ—ä¸€å¥³æ ¡ç¶²æœ€æ–°æ¶ˆæ¯ä¸­æœå°‹ï¼‰", "ä¾‹å¦‚ï¼šç•¢æ¥­å…¸ç¦®")

if st.button("ç”Ÿæˆå›ç­”"):
    with st.spinner('æ­£åœ¨æœå°‹èˆ‡ç”Ÿæˆå›è¦†...'):
        response = generate_response_combined(task, keyword)
        st.success('è™•ç†å®Œæˆï¼')
        st.markdown(response)

st.markdown("---")

st.markdown("""
<style>
.button-container {
    display: flex;
    justify-content: flex-start;
    gap: 0.5rem;
    margin-top: 1rem;
}

a.fake-button {
    background-color: #f0f2f6;
    color: black;
    padding: 0.4rem 1rem;
    text-decoration: none;
    border-radius: 0.5rem;
    border: 1px solid #d3d3d3;
    font-weight: 500;
    font-size: 1rem;
    display: inline-block;
    transition: background-color 0.2s ease;
}

a.fake-button:hover {
    background-color: #e0e0e0;
}
</style>

<div class="button-container">
    <a href="https://christinechen0930.github.io/TFGquestionary/TFGQA.html" target="_blank" class="fake-button">ğŸ” å‰å¾€åŒ—ä¸€å¥³ä¸­å•ç­”é›†</a>
    <a href="https://christinechen0930.github.io/TFGquestionary/TFGhistory.html" target="_blank" class="fake-button">ğŸ“œ ç­è§£åŒ—ä¸€å¥³æ ¡å²</a>
</div>
""", unsafe_allow_html=True)
